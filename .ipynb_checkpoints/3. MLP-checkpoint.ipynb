{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Layer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "We will be using [Covertype dataset](https://archive.ics.uci.edu/ml/datasets/covertype) available in dataset folder to work with MLP.\n",
    "<br>We are supposed to predict `Forest Cover type` using following features\n",
    "\n",
    "<img src=\"img/Dataset.png\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make + as string concat operator as well.. Dont worry about this cell...\n",
    "\"+\" = function(x,y) { if(is.character(x) || is.character(y)) return(paste(x , y, sep=\"\")) else .Primitive(\"+\")(x,y) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"package 'keras' was built under R version 4.1.1\"\n"
     ]
    }
   ],
   "source": [
    "library(\"keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = read.csv(\"dataset/covtype.data\", header=FALSE, stringsAsFactors=FALSE)\n",
    "colnames(d1) = c('Elevation', 'Aspect', 'Slope', 'HorHydro', 'VertHydro', 'HorRoad', 'Shade9', 'Shade12', 'Shade15', 'HorFire', 'WildernessArea'+(1:4), 'SoilType'+(1:40), 'CoverType')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data  =  d1[  , -ncol(d1) ]\n",
    "train_labels = d1[  ,  ncol(d1) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input Data Format\n",
    "Keras requires the input data to be matrix with features as columns and instance as rows.\n",
    "<br>The response variable is required to be a binary matrix with each class in a seraparate column, which is achieved using `to_categorical` function of keras. `to_categorical` requires the labels to start from `0`. But since in our dataset it start with `1` we, subtract its value by 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = as.matrix(train_data)\n",
    "train_labels = to_categorical(train_labels-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/Network.PNG\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Stack of Layers\n",
    "Keras Model composed of Layers of compute units, most common of which is a Linear Stack of Layer.\n",
    "<br> We define a Linear stack using \"keras_model_sequential\" functions.\n",
    "<br> We then add layer by layer to the stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras_model_sequential() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input Layer\n",
    "The first layer of MLP is the input layer which receives the input data.\n",
    "<br> The total number of nodes in this layer is equal to the total number of input features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hidden Layer\n",
    "Hidden layer is where the computation takes places. There are many parameters in hidden layer which changes the performance of the model, few of which are `activation function`, `number of hidden layers in a nerwork`, `number of hidden units in a layer`. The below shows an example with 2 hidden layers and 200 units in each hidden layer, with `relu` activation function for each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model  %>% layer_dense(\n",
    "    input_shape = ncol(train_data), \n",
    "    units = 200, \n",
    "    activation = 'relu',\n",
    "    name=\"layer1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model  %>% layer_dense(\n",
    "    units = 200, \n",
    "    activation = 'relu',\n",
    "    name=\"layer2\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output Layer\n",
    "Output layer is the last layer of the network, which gives the prediction.\n",
    "<br> The nature of the `activation function` tells if the prediction is classification or regression. For classification there are couple of options, with `softmax` being one of them for multi-class classification. \n",
    "<br> Total number of units in output class should be equal to the total number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model  %>% layer_dense(units = ncol(train_labels), activation = 'softmax', name=\"layer3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating total number of model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing model shows the following. Below is the calculation for total number of parameters for each layer.<br>\n",
    "- Since the 1st layer (input layer) is connected to input, total number of parameter = number of input columns(54) X number of units in 1st layer (200) + 200 bias (1 for each node) = 11000 parameters <br>\n",
    "- Since the 2nd layer (hidden layer) is connected to input layer, total number of parameter = number of units in 1st layer (200) X number of units in 2nd layer (200) + 200 bias (1 for each node) = 40200 parameters <br>\n",
    "- Since the 3rd layer (output layer) is connected to input layer, total number of parameter = number of units in 2nd layer (200) X number of units in 3rd layer (7) + 7 bias (1 for each node) = 1407 parameters <br>\n",
    "- Total number of parameters is the sum of all the parameters in each layer -- provided if a layer weights are set to non-trainable\n",
    "\n",
    "<br>\n",
    "<div>\n",
    "<img src=\"img/Model.png\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure Loss Function, Optimizer Algorithm, Error Metric using `compile` function\n",
    "\n",
    "- Loss Functions - https://keras.io/api/losses/\n",
    "- Optimizer Algorithms - https://keras.io/api/optimizers/\n",
    "- Error Metrics - https://keras.io/api/metrics/ \n",
    "\n",
    "compile R function - https://keras.rstudio.com/reference/compile.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model %>% compile(\n",
    "    loss = 'categorical_crossentropy',\n",
    "    optimizer = optimizer_adam(),\n",
    "    metrics = 'accuracy'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit Model to Training Data\n",
    "\n",
    "When fitting the model you get to choose \n",
    "- batch size\n",
    "- epochs\n",
    "- validation split\n",
    "- class weight\n",
    "- ... \n",
    "\n",
    "The arguments of the `fit` function are elaborated in the below link\n",
    "https://keras.rstudio.com/reference/fit.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history <- fit(\n",
    "    object = model,\n",
    "    x = train_data,\n",
    "    y = train_labels,\n",
    "    epochs = 25,\n",
    "    validation_split = 0.2\n",
    ")\n",
    "\n",
    "plot(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output of De-Normalized Data\n",
    "\n",
    "Below is the output of the denormalized data. Ideally MLP requires the data to be normalized between 0 and 1 for it to be efficient. If the data is not normalized then it will take many epochs for the model to converge to reasonably optimum weights, which will require more compute capacity. Also if the scale between the features are vastly different, chances are that it might get held up in a local minima far from the global minima.\n",
    "\n",
    "<img src=\"img/denorm.png\" width=\"400\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance of MLP with Normalized Input Data\n",
    "\n",
    "On check the summary of the training data, it can be observed that the 1st ten columns' scale is very large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way of normalizing is by z-score -> ($x-\\mu) / \\sigma$ <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2 = d1 \n",
    "\n",
    "means1 = sapply(d1[,1:10], mean)\n",
    "sd1 = sapply(d1[,1:10], sd)\n",
    "\n",
    "d2[ , 1:10 ] = t((t(d1[ , 1:10 ]) - means1)  / sd1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rest of the steps follow as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data  =  d2[  , -ncol(d2) ]\n",
    "train_labels = d2[  ,  ncol(d2) ]\n",
    "\n",
    "train_data = as.matrix(train_data)\n",
    "train_labels = to_categorical(train_labels-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model <- keras_model_sequential()  %>% \n",
    "\n",
    "layer_dense(\n",
    "    input_shape = ncol(train_data), \n",
    "    units = 200, \n",
    "    activation = 'relu'\n",
    ") %>% \n",
    "\n",
    "layer_dense(\n",
    "    units = 200, \n",
    "    activation = 'relu'\n",
    ") %>% \n",
    "\n",
    "layer_dense(units = ncol(train_labels), activation = 'softmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model %>% compile(\n",
    "    loss = 'categorical_crossentropy',\n",
    "    optimizer = optimizer_adam(),\n",
    "    metrics = 'accuracy'\n",
    ")\n",
    "\n",
    "history <- fit(\n",
    "    object = model,\n",
    "    x = train_data,\n",
    "    y = train_labels,\n",
    "    epochs = 25,\n",
    "    validation_split = 0.2\n",
    ")\n",
    "\n",
    "plot(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output of Normalized Data\n",
    "\n",
    "It can be observed that the model converges faster to optimum than de-normalized data.\n",
    "There are many methods of normalization. [This article](https://developers.google.com/machine-learning/data-prep/transform/normalization) provides a good explanation of the available methods.\n",
    "\n",
    "<div>\n",
    "<img src=\"img/denorm.png\" width=\"400\" />\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More Hidden Layer\n",
    "Deeper neural networks can help predict complex patterns. Below code works has same models parameters of the previous except for an additional hidden layer of 200 units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model <- keras_model_sequential()  %>% \n",
    "\n",
    "layer_dense(\n",
    "    input_shape = ncol(train_data), \n",
    "    units = 200, \n",
    "    activation = 'relu'\n",
    ") %>% \n",
    "\n",
    "############ added layer ############\n",
    "\n",
    "layer_dense(\n",
    "    units = 200, \n",
    "    activation = 'relu'\n",
    ") %>% \n",
    "\n",
    "############ added layer ############\n",
    "\n",
    "layer_dense(\n",
    "    units = 200, \n",
    "    activation = 'relu'\n",
    ") %>% \n",
    "\n",
    "layer_dense(units = ncol(train_labels), activation = 'softmax')\n",
    "\n",
    "model %>% compile(\n",
    "    loss = 'categorical_crossentropy',\n",
    "    optimizer = optimizer_adam(),\n",
    "    metrics = 'accuracy'\n",
    ")\n",
    "\n",
    "\n",
    "model %>% compile(\n",
    "    loss = 'categorical_crossentropy',\n",
    "    optimizer = optimizer_adam(),\n",
    "    metrics = 'accuracy'\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "history <- fit(\n",
    "    object = model,\n",
    "    x = train_data,\n",
    "    y = train_labels,\n",
    "    epochs = 25,\n",
    "    validation_split = 0.2\n",
    ")\n",
    "\n",
    "plot(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of test and validation set has improved compared to model with 1 hidden layer.\n",
    "Having said, the performance of the model is not proportional to the number of hidden layers / units in each hidden unit. \n",
    "<img src=\"img/TwoHidden.png\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch Size\n",
    "Batch size defines the number of instances after which the weights are updated.\n",
    "<br>If the batch size of 1 then after each instance, the weight is updated. If the batch size is equal to size of dataset then the weights are updated once for each epoch.\n",
    "<br>The default batch size is 32.\n",
    "<br> Higher the batch size, faster is the execution with GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ same as before ############\n",
    "model <- keras_model_sequential()  %>% \n",
    "\n",
    "layer_dense(\n",
    "    input_shape = ncol(train_data), \n",
    "    units = 200, \n",
    "    activation = 'relu'\n",
    ") %>% \n",
    "\n",
    "layer_dense(\n",
    "    units = 200, \n",
    "    activation = 'relu'\n",
    ") %>% \n",
    "\n",
    "layer_dense(units = ncol(train_labels), activation = 'softmax')\n",
    "\n",
    "############ same as before ############\n",
    "\n",
    "model %>% compile(\n",
    "    loss = 'categorical_crossentropy',\n",
    "    optimizer = optimizer_adam(),\n",
    "    metrics = 'accuracy'\n",
    ")\n",
    "\n",
    "model %>% compile(\n",
    "    loss = 'categorical_crossentropy',\n",
    "    optimizer = optimizer_adam(),\n",
    "    metrics = 'accuracy'\n",
    ")\n",
    "\n",
    "############ same as before ############\n",
    "\n",
    "history <- fit(\n",
    "    object = model,\n",
    "    x = train_data,\n",
    "    y = train_labels,\n",
    "    epochs = 25,\n",
    "    validation_split = 0.2,\n",
    "    batch_size=nrow(train_data) # set the batch size to size of dataset\n",
    ")\n",
    "\n",
    "plot(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result shows that large batch size takes more epoch to converge. Ideally with smaller batch size there would be greater variation in error across each epoch, however it would reach higher accuracy/performance with fewer epochs, on the other hand if the batch size is very large then the variation of model performance across epochs is small after convergence, but it takes longer to converge.\n",
    "\n",
    "<div>\n",
    "<img src=\"img/BatchSizeMax.png\" width=\"400\"/>\n",
    "</div>\n",
    "\n",
    "[This link](https://machinelearningmastery.com/how-to-control-the-speed-and-stability-of-training-neural-networks-with-gradient-descent-batch-size/) has good information on the pros and cons of varying batch size, from where the below plot has been taken from showing the performance of model on mnist data for varying batch size.\n",
    "\n",
    "<div>\n",
    "<img src=\"img/BatchSizeWebSite.png\" width=\"400\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning Rate\n",
    "Batch Size and learning rate are usually tuned together. Smaller learning rate takes more epochs to converge, whereas bigger learning rate can cause model to jump around the optimal point. Since large batch size takes more number of epochs to reach close to convergence, it is a usual practice that learning rate is increased with batch size. However, this can depend on specific requirement. <br>\n",
    "Below is shown a model with learning rate 100 times less than the default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ same as before ############\n",
    "model <- keras_model_sequential()  %>% \n",
    "\n",
    "layer_dense(\n",
    "    input_shape = ncol(train_data), \n",
    "    units = 200, \n",
    "    activation = 'relu'\n",
    ") %>% \n",
    "\n",
    "layer_dense(\n",
    "    units = 200, \n",
    "    activation = 'relu'\n",
    ") %>% \n",
    "\n",
    "layer_dense(units = ncol(train_labels), activation = 'softmax')\n",
    "\n",
    "\n",
    "model %>% compile(\n",
    "    loss = 'categorical_crossentropy',\n",
    "    optimizer = optimizer_adam(lr=0.00001),\n",
    "    metrics = 'accuracy'\n",
    ")\n",
    "\n",
    "\n",
    "############ same as before ############\n",
    "\n",
    "model %>% compile(\n",
    "    loss = 'categorical_crossentropy',\n",
    "    optimizer = optimizer_adam(),\n",
    "    metrics = 'accuracy'\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "history <- fit(\n",
    "    object = model,\n",
    "    x = train_data,\n",
    "    y = train_labels,\n",
    "    epochs = 25,\n",
    "    validation_split = 0.2,\n",
    "    batch_size=nrow(train_data)\n",
    ")\n",
    "\n",
    "plot(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With learning rate 100 times less than defauly it can be observed that even after 25 epochs, the model doesn't seems to be anywhere close to the optimum because the steps taken by weights across each epoch is very small.\n",
    "\n",
    "<img src=\"img/LearningRate.png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression\n",
    "\n",
    "For regression, the main changes in parameters include\n",
    "- activation function of the output layer changes (Eg. linear)\n",
    "- loss function (Eg. mean square error)\n",
    "- error metric (Eg. mean absolute error)\n",
    "\n",
    "For demo, lets predict Elevation using all the other features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "d2 = d1 \n",
    "\n",
    "means1 = sapply(d1[,1:10], mean)\n",
    "sd1 = sapply(d1[,1:10], sd)\n",
    "\n",
    "d2[ , 1:10 ] = t((t(d1[ , 1:10 ]) - means1)  / sd1)\n",
    "\n",
    "# Converting Cover Type into Binary Matrix\n",
    "d2 = cbind(\n",
    "                    d2[  , -ncol(d2) ],  \n",
    "    to_categorical( d2[  ,  ncol(d2) ] - 1 )\n",
    ")\n",
    "\n",
    "\n",
    "train_data  =  d2[  , -1 ]\n",
    "train_labels = d2[  ,  1 ]\n",
    "\n",
    "train_data = as.matrix(train_data)\n",
    "train_labels = as.matrix(train_labels)\n",
    "\n",
    "\n",
    "\n",
    "# Changing Output Activation to Linear\n",
    "\n",
    "model <- keras_model_sequential()  %>% \n",
    "\n",
    "layer_dense(\n",
    "    input_shape = ncol(train_data), \n",
    "    units = 200, \n",
    "    activation = 'relu',\n",
    "    name=\"layer1\"\n",
    ") %>% \n",
    "\n",
    "layer_dense(\n",
    "    units = 200, \n",
    "    activation = 'relu',\n",
    "    name=\"layer2\"\n",
    ") %>% \n",
    "\n",
    "layer_dense(units = ncol(train_labels), activation = 'linear', name=\"layer3\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Changing Loss Function to Mean Square Error, and error metric to Mean Absolute Error\n",
    "model %>% compile(\n",
    "    loss = 'mse',\n",
    "    optimizer = optimizer_adam(),\n",
    "    metrics = 'mean_absolute_error'\n",
    ")\n",
    "\n",
    "# Setting Batch Size very high for faster computation\n",
    "history <- fit(\n",
    "    object = model,\n",
    "    x = train_data,\n",
    "    y = train_labels,\n",
    "    epochs = 100,\n",
    "    validation_split = 0.2,\n",
    "    batch_size=nrow(train_data)\n",
    ")\n",
    "\n",
    "plot(history)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/Regression.png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
